{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5433e97e15c147cbae3cf4227a4c6d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2112cfa71b7449ce9d7cbf63f3b4d344",
              "IPY_MODEL_0241411dc66540119339250bed98456f",
              "IPY_MODEL_940d16486e09400b9fe773e47541f0dd"
            ],
            "layout": "IPY_MODEL_b3d70128041f4ac1baa5041539cdebb6"
          }
        },
        "2112cfa71b7449ce9d7cbf63f3b4d344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e034f1b89563416cb0a15980e904d3bc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6fa06584534c43c28267ea11fab2a8da",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "0241411dc66540119339250bed98456f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5caadbe1487f44cc87219e50e6b518b8",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_396431f78a894adba9a025acac893e72",
            "value": 346284714
          }
        },
        "940d16486e09400b9fe773e47541f0dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_306dde9a46954691ba5b76cb4efef1b8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e8751895886a4fa69f5aaf743a70c8d5",
            "value": "â€‡346M/346Mâ€‡[00:05&lt;00:00,â€‡178MB/s]"
          }
        },
        "b3d70128041f4ac1baa5041539cdebb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e034f1b89563416cb0a15980e904d3bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fa06584534c43c28267ea11fab2a8da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5caadbe1487f44cc87219e50e6b518b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "396431f78a894adba9a025acac893e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "306dde9a46954691ba5b76cb4efef1b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8751895886a4fa69f5aaf743a70c8d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1 â€” Vision Transformer on CIFAR-10 (PyTorch)\n",
        "\n",
        "## Goal\n",
        "Implement a Vision Transformer (ViT) and train it on **CIFAR-10 (10 classes)**.  \n",
        "Your objective is to achieve the **highest possible test accuracy**.  \n",
        "\n",
        "You are free to experiment with improvements and tricks to push performance further.  \n",
        "**Note:** You must use **Google Colab** for implementation.  \n",
        "\n",
        "ðŸ“„ Reference Paper:  \n",
        "*Dosovitskiy et al., \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\", ICLR 2021*  \n",
        "[Paper Link](https://arxiv.org/abs/2010.11929)\n",
        "\n",
        "---\n",
        "\n",
        "## Requirements\n",
        "- Patchify images  \n",
        "- Add learnable positional embeddings  \n",
        "- Prepend a **CLS token**  \n",
        "- Stack Transformer encoder blocks:  \n",
        "  - Multi-Head Self Attention (MHSA)  \n",
        "  - MLP with residual connections + normalization  \n",
        "- Classify from the **CLS token**  \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Bonus (Optional Analysis)\n",
        "You can earn bonus marks by including a **concise analysis**. Keep it **short and crisp**.  \n",
        "Some examples of analysis directions:\n",
        "- Choice of patch size  \n",
        "- Depth vs. width trade-offs  \n",
        "- Effect of data augmentation  \n",
        "- Optimizer and learning schedule variants  \n",
        "- Overlapping vs. non-overlapping patches  \n",
        "\n",
        "This analysis should also be part of your `README.md`.  \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "3vByTUCciVGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- SETUP ---\n",
        "!pip install timm torchsummary\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import timm\n",
        "from timm.loss import LabelSmoothingCrossEntropy\n",
        "from timm.utils import ModelEmaV2\n"
      ],
      "metadata": {
        "id": "8uJghupuKvkQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7565f802-36d9-499f-9cf4-95ce5fdf015b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.20)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.12/dist-packages (1.5.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.23.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.35.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- CONFIG ---\n",
        "class CFG:\n",
        "    model_name = \"vit_base_patch16_224\"  # pretrained ViT\n",
        "    img_size = 224\n",
        "    batch_size = 64\n",
        "    epochs = 30\n",
        "    lr = 5e-5\n",
        "    weight_decay = 0.05\n",
        "    num_classes = 10\n",
        "    smoothing = 0.1\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "cfg = CFG()"
      ],
      "metadata": {
        "id": "lo3UizqRKz2z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- DATASET & AUGMENTATIONS ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((cfg.img_size, cfg.img_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])"
      ],
      "metadata": {
        "id": "9IuXtKxMK4dY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((cfg.img_size, cfg.img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n"
      ],
      "metadata": {
        "id": "U1VyLvkcK6ZG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "trainloader = DataLoader(trainset, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n"
      ],
      "metadata": {
        "id": "3ezaSol4K9RK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6762cab5-7870-4b9e-da91-0c8ff5572837"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:24<00:00, 7.00MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=val_transform)\n",
        "valloader = DataLoader(valset, batch_size=cfg.batch_size, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "vTmUk2nbK_dY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MODEL ---\n",
        "model = timm.create_model(cfg.model_name, pretrained=True, num_classes=cfg.num_classes)\n",
        "model.to(cfg.device)\n"
      ],
      "metadata": {
        "id": "MCqkBQ_6LCbg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5433e97e15c147cbae3cf4227a4c6d96",
            "2112cfa71b7449ce9d7cbf63f3b4d344",
            "0241411dc66540119339250bed98456f",
            "940d16486e09400b9fe773e47541f0dd",
            "b3d70128041f4ac1baa5041539cdebb6",
            "e034f1b89563416cb0a15980e904d3bc",
            "6fa06584534c43c28267ea11fab2a8da",
            "5caadbe1487f44cc87219e50e6b518b8",
            "396431f78a894adba9a025acac893e72",
            "306dde9a46954691ba5b76cb4efef1b8",
            "e8751895886a4fa69f5aaf743a70c8d5"
          ]
        },
        "outputId": "08bf7e64-23fd-4692-c6ff-f399dc70ca78"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5433e97e15c147cbae3cf4227a4c6d96"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (patch_embed): PatchEmbed(\n",
              "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "    (norm): Identity()\n",
              "  )\n",
              "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "  (patch_drop): Identity()\n",
              "  (norm_pre): Identity()\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (1): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (2): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (3): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (4): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (5): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (6): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (7): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (8): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (9): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (10): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (11): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  (fc_norm): Identity()\n",
              "  (head_drop): Dropout(p=0.0, inplace=False)\n",
              "  (head): Linear(in_features=768, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- LOSS & OPTIMIZER ---\n",
        "criterion = LabelSmoothingCrossEntropy(smoothing=cfg.smoothing)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n"
      ],
      "metadata": {
        "id": "M09jH1Z9LGHe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EMA for stability\n",
        "ema = ModelEmaV2(model, decay=0.999)\n"
      ],
      "metadata": {
        "id": "unQR_w8BLJsV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TRAIN & EVAL ---\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device, ema):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for i, (images, targets) in enumerate(loader):\n",
        "        images, targets = images.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        ema.update(model)\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if (i + 1) % 100 == 0: # Print every 100 batches\n",
        "            print(f\"  Batch {i+1}/{len(loader)} | Loss: {loss.item():.4f} | Acc: {100.*correct/total:.2f}%\")\n",
        "\n",
        "    return total_loss/total, 100.*correct/total"
      ],
      "metadata": {
        "id": "o3MZhtvDLN9s"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "FZrPrnlRTAE5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TRAIN & EVAL ---\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device, ema, log_interval=50):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    start_epoch_time = time.time()\n",
        "    for batch_idx, (images, targets) in enumerate(loader):\n",
        "        batch_start = time.time()\n",
        "\n",
        "        # Data transfer\n",
        "        data_start = time.time()\n",
        "        images, targets = images.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
        "        data_time = time.time() - data_start\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        fwd_start = time.time()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "        fwd_time = time.time() - fwd_start\n",
        "\n",
        "        # Backward pass\n",
        "        bwd_start = time.time()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        ema.update(model)\n",
        "        bwd_time = time.time() - bwd_start\n",
        "\n",
        "        # Metrics\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        batch_time = time.time() - batch_start\n",
        "\n",
        "        # Logging every few batches\n",
        "        if (batch_idx + 1) % log_interval == 0:\n",
        "            print(f\"Batch {batch_idx+1}/{len(loader)} | \"\n",
        "                  f\"Loss: {loss.item():.4f} | \"\n",
        "                  f\"Data: {data_time:.3f}s | \"\n",
        "                  f\"Fwd: {fwd_time:.3f}s | \"\n",
        "                  f\"Bwd: {bwd_time:.3f}s | \"\n",
        "                  f\"Batch: {batch_time:.3f}s | \"\n",
        "                  f\"Acc so far: {100.*correct/total:.2f}%\")\n",
        "\n",
        "    epoch_time = time.time() - start_epoch_time\n",
        "    print(f\"Epoch finished in {epoch_time:.2f}s\")\n",
        "    return total_loss/total, 100.*correct/total\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion, device, log_interval=50):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    start_eval_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, targets) in enumerate(loader):\n",
        "            images, targets = images.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            if (batch_idx + 1) % log_interval == 0:\n",
        "                print(f\"[Eval] Batch {batch_idx+1}/{len(loader)} | \"\n",
        "                      f\"Loss: {loss.item():.4f} | \"\n",
        "                      f\"Acc so far: {100.*correct/total:.2f}%\")\n",
        "\n",
        "    eval_time = time.time() - start_eval_time\n",
        "    print(f\"Evaluation finished in {eval_time:.2f}s\")\n",
        "    return total_loss/total, 100.*correct/total\n"
      ],
      "metadata": {
        "id": "Xr2rArnvLQjv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- TRAINING LOOP ---\n",
        "for epoch in range(cfg.epochs):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{cfg.epochs} ---\")\n",
        "    train_loss, train_acc = train_one_epoch(model, trainloader, optimizer, criterion, cfg.device, ema)\n",
        "    val_loss, val_acc = evaluate(model, valloader, criterion, cfg.device)\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch+1}/{cfg.epochs} | \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "HwYiVNYnLT_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b366ca2-55a8-4dcb-f4e0-670a6b606517"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Epoch 1/30 ---\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-937707582.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 50/782 | Loss: 0.5728 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.202s | Batch: 0.485s | Acc so far: 97.34%\n",
            "Batch 100/782 | Loss: 0.5183 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.197s | Batch: 0.469s | Acc so far: 97.05%\n",
            "Batch 150/782 | Loss: 0.5460 | Data: 0.011s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.471s | Acc so far: 96.74%\n",
            "Batch 200/782 | Loss: 0.5825 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.210s | Batch: 0.483s | Acc so far: 96.73%\n",
            "Batch 250/782 | Loss: 0.5986 | Data: 0.013s | Fwd: 0.018s | Bwd: 0.198s | Batch: 0.478s | Acc so far: 96.62%\n",
            "Batch 300/782 | Loss: 0.6176 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.472s | Acc so far: 96.60%\n",
            "Batch 350/782 | Loss: 0.5934 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.473s | Acc so far: 96.49%\n",
            "Batch 400/782 | Loss: 0.6353 | Data: 0.020s | Fwd: 0.014s | Bwd: 0.203s | Batch: 0.482s | Acc so far: 96.48%\n",
            "Batch 450/782 | Loss: 0.5934 | Data: 0.023s | Fwd: 0.026s | Bwd: 0.191s | Batch: 0.485s | Acc so far: 96.43%\n",
            "Batch 500/782 | Loss: 0.6195 | Data: 0.021s | Fwd: 0.027s | Bwd: 0.192s | Batch: 0.489s | Acc so far: 96.43%\n",
            "Batch 550/782 | Loss: 0.5416 | Data: 0.020s | Fwd: 0.030s | Bwd: 0.186s | Batch: 0.479s | Acc so far: 96.41%\n",
            "Batch 600/782 | Loss: 0.6752 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.202s | Batch: 0.473s | Acc so far: 96.45%\n",
            "Batch 650/782 | Loss: 0.5503 | Data: 0.021s | Fwd: 0.025s | Bwd: 0.192s | Batch: 0.482s | Acc so far: 96.42%\n",
            "Batch 700/782 | Loss: 0.5488 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.204s | Batch: 0.478s | Acc so far: 96.40%\n",
            "Batch 750/782 | Loss: 0.5484 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.472s | Acc so far: 96.40%\n",
            "Epoch finished in 382.20s\n",
            "[Eval] Batch 50/157 | Loss: 0.6361 | Acc so far: 96.91%\n",
            "[Eval] Batch 100/157 | Loss: 0.5955 | Acc so far: 96.78%\n",
            "[Eval] Batch 150/157 | Loss: 0.6707 | Acc so far: 96.84%\n",
            "Evaluation finished in 106.77s\n",
            "Epoch 1/30 | Train Loss: 0.5886, Train Acc: 96.42% | Val Loss: 0.5794, Val Acc: 96.87%\n",
            "\n",
            "--- Epoch 2/30 ---\n",
            "Batch 50/782 | Loss: 0.5334 | Data: 0.012s | Fwd: 0.018s | Bwd: 0.199s | Batch: 0.472s | Acc so far: 96.88%\n",
            "Batch 100/782 | Loss: 0.6198 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.472s | Acc so far: 96.69%\n",
            "Batch 150/782 | Loss: 0.5580 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.475s | Acc so far: 96.72%\n",
            "Batch 200/782 | Loss: 0.6153 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.474s | Acc so far: 96.77%\n",
            "Batch 250/782 | Loss: 0.5407 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.471s | Acc so far: 96.73%\n",
            "Batch 300/782 | Loss: 0.6035 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.202s | Batch: 0.472s | Acc so far: 96.67%\n",
            "Batch 350/782 | Loss: 0.6609 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.475s | Acc so far: 96.64%\n",
            "Batch 400/782 | Loss: 0.5284 | Data: 0.012s | Fwd: 0.017s | Bwd: 0.197s | Batch: 0.469s | Acc so far: 96.73%\n",
            "Batch 450/782 | Loss: 0.5305 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.202s | Batch: 0.473s | Acc so far: 96.69%\n",
            "Batch 500/782 | Loss: 0.5824 | Data: 0.017s | Fwd: 0.017s | Bwd: 0.197s | Batch: 0.478s | Acc so far: 96.70%\n",
            "Batch 550/782 | Loss: 0.5787 | Data: 0.014s | Fwd: 0.021s | Bwd: 0.197s | Batch: 0.478s | Acc so far: 96.60%\n",
            "Batch 600/782 | Loss: 0.5545 | Data: 0.022s | Fwd: 0.027s | Bwd: 0.189s | Batch: 0.485s | Acc so far: 96.62%\n",
            "Batch 650/782 | Loss: 0.5464 | Data: 0.021s | Fwd: 0.035s | Bwd: 0.183s | Batch: 0.480s | Acc so far: 96.67%\n",
            "Batch 700/782 | Loss: 0.5671 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.473s | Acc so far: 96.67%\n",
            "Batch 750/782 | Loss: 0.5422 | Data: 0.012s | Fwd: 0.017s | Bwd: 0.199s | Batch: 0.472s | Acc so far: 96.66%\n",
            "Epoch finished in 381.01s\n",
            "[Eval] Batch 50/157 | Loss: 0.5700 | Acc so far: 97.50%\n",
            "[Eval] Batch 100/157 | Loss: 0.5410 | Acc so far: 97.38%\n",
            "[Eval] Batch 150/157 | Loss: 0.5442 | Acc so far: 97.35%\n",
            "Evaluation finished in 106.57s\n",
            "Epoch 2/30 | Train Loss: 0.5806, Train Acc: 96.69% | Val Loss: 0.5678, Val Acc: 97.40%\n",
            "\n",
            "--- Epoch 3/30 ---\n",
            "Batch 50/782 | Loss: 0.6251 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.204s | Batch: 0.478s | Acc so far: 96.88%\n",
            "Batch 100/782 | Loss: 0.5941 | Data: 0.012s | Fwd: 0.017s | Bwd: 0.204s | Batch: 0.477s | Acc so far: 97.36%\n",
            "Batch 150/782 | Loss: 0.5397 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.471s | Acc so far: 97.35%\n",
            "Batch 200/782 | Loss: 0.5831 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.472s | Acc so far: 97.27%\n",
            "Batch 250/782 | Loss: 0.5691 | Data: 0.012s | Fwd: 0.017s | Bwd: 0.198s | Batch: 0.469s | Acc so far: 97.15%\n",
            "Batch 300/782 | Loss: 0.6849 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.474s | Acc so far: 97.10%\n",
            "Batch 350/782 | Loss: 0.5555 | Data: 0.014s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.477s | Acc so far: 97.08%\n",
            "Batch 400/782 | Loss: 0.5817 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.471s | Acc so far: 97.07%\n",
            "Batch 450/782 | Loss: 0.5262 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.470s | Acc so far: 97.11%\n",
            "Batch 500/782 | Loss: 0.5676 | Data: 0.011s | Fwd: 0.018s | Bwd: 0.198s | Batch: 0.473s | Acc so far: 97.11%\n",
            "Batch 550/782 | Loss: 0.5589 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.475s | Acc so far: 97.09%\n",
            "Batch 600/782 | Loss: 0.5665 | Data: 0.023s | Fwd: 0.030s | Bwd: 0.187s | Batch: 0.484s | Acc so far: 97.11%\n",
            "Batch 650/782 | Loss: 0.6771 | Data: 0.021s | Fwd: 0.033s | Bwd: 0.181s | Batch: 0.482s | Acc so far: 97.06%\n",
            "Batch 700/782 | Loss: 0.6015 | Data: 0.024s | Fwd: 0.027s | Bwd: 0.186s | Batch: 0.482s | Acc so far: 97.05%\n",
            "Batch 750/782 | Loss: 0.6027 | Data: 0.014s | Fwd: 0.022s | Bwd: 0.193s | Batch: 0.476s | Acc so far: 97.04%\n",
            "Epoch finished in 380.75s\n",
            "[Eval] Batch 50/157 | Loss: 0.6803 | Acc so far: 97.00%\n",
            "[Eval] Batch 100/157 | Loss: 0.5534 | Acc so far: 97.22%\n",
            "[Eval] Batch 150/157 | Loss: 0.5470 | Acc so far: 97.29%\n",
            "Evaluation finished in 106.52s\n",
            "Epoch 3/30 | Train Loss: 0.5726, Train Acc: 97.00% | Val Loss: 0.5688, Val Acc: 97.31%\n",
            "\n",
            "--- Epoch 4/30 ---\n",
            "Batch 50/782 | Loss: 0.5652 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.202s | Batch: 0.473s | Acc so far: 97.16%\n",
            "Batch 100/782 | Loss: 0.6703 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.472s | Acc so far: 97.33%\n",
            "Batch 150/782 | Loss: 0.5579 | Data: 0.013s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.475s | Acc so far: 97.24%\n",
            "Batch 200/782 | Loss: 0.5819 | Data: 0.013s | Fwd: 0.017s | Bwd: 0.202s | Batch: 0.478s | Acc so far: 97.26%\n",
            "Batch 250/782 | Loss: 0.6262 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.473s | Acc so far: 97.21%\n",
            "Batch 300/782 | Loss: 0.5604 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.471s | Acc so far: 97.24%\n",
            "Batch 350/782 | Loss: 0.5100 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.467s | Acc so far: 97.26%\n",
            "Batch 400/782 | Loss: 0.5684 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.205s | Batch: 0.476s | Acc so far: 97.29%\n",
            "Batch 450/782 | Loss: 0.5338 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.471s | Acc so far: 97.31%\n",
            "Batch 500/782 | Loss: 0.5530 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.470s | Acc so far: 97.28%\n",
            "Batch 550/782 | Loss: 0.5274 | Data: 0.022s | Fwd: 0.035s | Bwd: 0.178s | Batch: 0.475s | Acc so far: 97.35%\n",
            "Batch 600/782 | Loss: 0.5786 | Data: 0.012s | Fwd: 0.021s | Bwd: 0.197s | Batch: 0.476s | Acc so far: 97.39%\n",
            "Batch 650/782 | Loss: 0.6071 | Data: 0.024s | Fwd: 0.027s | Bwd: 0.188s | Batch: 0.481s | Acc so far: 97.34%\n",
            "Batch 700/782 | Loss: 0.6419 | Data: 0.023s | Fwd: 0.028s | Bwd: 0.187s | Batch: 0.483s | Acc so far: 97.27%\n",
            "Batch 750/782 | Loss: 0.5791 | Data: 0.016s | Fwd: 0.023s | Bwd: 0.194s | Batch: 0.474s | Acc so far: 97.23%\n",
            "Epoch finished in 380.13s\n",
            "[Eval] Batch 50/157 | Loss: 0.5648 | Acc so far: 97.19%\n",
            "[Eval] Batch 100/157 | Loss: 0.5669 | Acc so far: 97.53%\n",
            "[Eval] Batch 150/157 | Loss: 0.5839 | Acc so far: 97.44%\n",
            "Evaluation finished in 106.44s\n",
            "Epoch 4/30 | Train Loss: 0.5671, Train Acc: 97.24% | Val Loss: 0.5671, Val Acc: 97.43%\n",
            "\n",
            "--- Epoch 5/30 ---\n",
            "Batch 50/782 | Loss: 0.5712 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.198s | Batch: 0.469s | Acc so far: 97.00%\n",
            "Batch 100/782 | Loss: 0.5771 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.204s | Batch: 0.476s | Acc so far: 97.22%\n",
            "Batch 150/782 | Loss: 0.5572 | Data: 0.020s | Fwd: 0.018s | Bwd: 0.193s | Batch: 0.475s | Acc so far: 97.15%\n",
            "Batch 200/782 | Loss: 0.5454 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.472s | Acc so far: 97.23%\n",
            "Batch 250/782 | Loss: 0.5909 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.197s | Batch: 0.469s | Acc so far: 97.16%\n",
            "Batch 300/782 | Loss: 0.5306 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.471s | Acc so far: 97.16%\n",
            "Batch 350/782 | Loss: 0.5642 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.471s | Acc so far: 97.21%\n",
            "Batch 400/782 | Loss: 0.6504 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.470s | Acc so far: 97.25%\n",
            "Batch 450/782 | Loss: 0.5553 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.468s | Acc so far: 97.26%\n",
            "Batch 500/782 | Loss: 0.5126 | Data: 0.012s | Fwd: 0.018s | Bwd: 0.197s | Batch: 0.470s | Acc so far: 97.24%\n",
            "Batch 550/782 | Loss: 0.6147 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.471s | Acc so far: 97.26%\n",
            "Batch 600/782 | Loss: 0.5768 | Data: 0.013s | Fwd: 0.017s | Bwd: 0.197s | Batch: 0.470s | Acc so far: 97.22%\n",
            "Batch 650/782 | Loss: 0.5362 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.469s | Acc so far: 97.25%\n",
            "Batch 700/782 | Loss: 0.5410 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.473s | Acc so far: 97.23%\n",
            "Batch 750/782 | Loss: 0.5543 | Data: 0.024s | Fwd: 0.025s | Bwd: 0.189s | Batch: 0.484s | Acc so far: 97.22%\n",
            "Epoch finished in 379.66s\n",
            "[Eval] Batch 50/157 | Loss: 0.5751 | Acc so far: 97.75%\n",
            "[Eval] Batch 100/157 | Loss: 0.5156 | Acc so far: 97.48%\n",
            "[Eval] Batch 150/157 | Loss: 0.6249 | Acc so far: 97.45%\n",
            "Evaluation finished in 106.54s\n",
            "Epoch 5/30 | Train Loss: 0.5668, Train Acc: 97.22% | Val Loss: 0.5664, Val Acc: 97.47%\n",
            "\n",
            "--- Epoch 6/30 ---\n",
            "Batch 50/782 | Loss: 0.6381 | Data: 0.013s | Fwd: 0.028s | Bwd: 0.187s | Batch: 0.471s | Acc so far: 97.25%\n",
            "Batch 100/782 | Loss: 0.5169 | Data: 0.012s | Fwd: 0.017s | Bwd: 0.201s | Batch: 0.471s | Acc so far: 97.56%\n",
            "Batch 150/782 | Loss: 0.6225 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.470s | Acc so far: 97.66%\n",
            "Batch 200/782 | Loss: 0.5110 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.204s | Batch: 0.475s | Acc so far: 97.66%\n",
            "Batch 250/782 | Loss: 0.5217 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.474s | Acc so far: 97.65%\n",
            "Batch 300/782 | Loss: 0.5842 | Data: 0.012s | Fwd: 0.017s | Bwd: 0.200s | Batch: 0.472s | Acc so far: 97.66%\n",
            "Batch 350/782 | Loss: 0.5282 | Data: 0.013s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.469s | Acc so far: 97.67%\n",
            "Batch 400/782 | Loss: 0.5567 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.469s | Acc so far: 97.60%\n",
            "Batch 450/782 | Loss: 0.5301 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.470s | Acc so far: 97.57%\n",
            "Batch 500/782 | Loss: 0.5453 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.467s | Acc so far: 97.53%\n",
            "Batch 550/782 | Loss: 0.5427 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.203s | Batch: 0.473s | Acc so far: 97.59%\n",
            "Batch 600/782 | Loss: 0.6362 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.474s | Acc so far: 97.58%\n",
            "Batch 650/782 | Loss: 0.5556 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.472s | Acc so far: 97.54%\n",
            "Batch 700/782 | Loss: 0.5483 | Data: 0.024s | Fwd: 0.025s | Bwd: 0.188s | Batch: 0.483s | Acc so far: 97.52%\n",
            "Batch 750/782 | Loss: 0.5191 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.201s | Batch: 0.471s | Acc so far: 97.52%\n",
            "Epoch finished in 379.40s\n",
            "[Eval] Batch 50/157 | Loss: 0.5864 | Acc so far: 97.62%\n",
            "[Eval] Batch 100/157 | Loss: 0.5259 | Acc so far: 97.67%\n",
            "[Eval] Batch 150/157 | Loss: 0.6083 | Acc so far: 97.59%\n",
            "Evaluation finished in 106.50s\n",
            "Epoch 6/30 | Train Loss: 0.5603, Train Acc: 97.50% | Val Loss: 0.5635, Val Acc: 97.58%\n",
            "\n",
            "--- Epoch 7/30 ---\n",
            "Batch 50/782 | Loss: 0.5327 | Data: 0.021s | Fwd: 0.028s | Bwd: 0.189s | Batch: 0.482s | Acc so far: 97.50%\n",
            "Batch 100/782 | Loss: 0.5258 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.468s | Acc so far: 97.39%\n",
            "Batch 150/782 | Loss: 0.5809 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.468s | Acc so far: 97.50%\n",
            "Batch 200/782 | Loss: 0.5867 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.204s | Batch: 0.475s | Acc so far: 97.60%\n",
            "Batch 250/782 | Loss: 0.5109 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.470s | Acc so far: 97.69%\n",
            "Batch 300/782 | Loss: 0.5884 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.203s | Batch: 0.473s | Acc so far: 97.69%\n",
            "Batch 350/782 | Loss: 0.5860 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.470s | Acc so far: 97.64%\n",
            "Batch 400/782 | Loss: 0.5408 | Data: 0.013s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.469s | Acc so far: 97.68%\n",
            "Batch 450/782 | Loss: 0.6142 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.202s | Batch: 0.476s | Acc so far: 97.64%\n",
            "Batch 500/782 | Loss: 0.5159 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.205s | Batch: 0.476s | Acc so far: 97.63%\n",
            "Batch 550/782 | Loss: 0.5482 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.474s | Acc so far: 97.63%\n",
            "Batch 600/782 | Loss: 0.5160 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.469s | Acc so far: 97.65%\n",
            "Batch 650/782 | Loss: 0.5249 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.470s | Acc so far: 97.63%\n",
            "Batch 700/782 | Loss: 0.5392 | Data: 0.020s | Fwd: 0.014s | Bwd: 0.202s | Batch: 0.478s | Acc so far: 97.59%\n",
            "Batch 750/782 | Loss: 0.5229 | Data: 0.011s | Fwd: 0.012s | Bwd: 0.204s | Batch: 0.470s | Acc so far: 97.62%\n",
            "Epoch finished in 378.81s\n",
            "[Eval] Batch 50/157 | Loss: 0.6240 | Acc so far: 96.75%\n",
            "[Eval] Batch 100/157 | Loss: 0.5817 | Acc so far: 96.84%\n",
            "[Eval] Batch 150/157 | Loss: 0.5187 | Acc so far: 96.83%\n",
            "Evaluation finished in 106.54s\n",
            "Epoch 7/30 | Train Loss: 0.5572, Train Acc: 97.62% | Val Loss: 0.5782, Val Acc: 96.87%\n",
            "\n",
            "--- Epoch 8/30 ---\n",
            "Batch 50/782 | Loss: 0.5325 | Data: 0.012s | Fwd: 0.017s | Bwd: 0.200s | Batch: 0.474s | Acc so far: 97.66%\n",
            "Batch 100/782 | Loss: 0.5170 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.204s | Batch: 0.475s | Acc so far: 97.91%\n",
            "Batch 150/782 | Loss: 0.5115 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.202s | Batch: 0.472s | Acc so far: 98.00%\n",
            "Batch 200/782 | Loss: 0.5402 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.469s | Acc so far: 97.96%\n",
            "Batch 250/782 | Loss: 0.5747 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.472s | Acc so far: 98.00%\n",
            "Batch 300/782 | Loss: 0.5737 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.470s | Acc so far: 97.99%\n",
            "Batch 350/782 | Loss: 0.5059 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.468s | Acc so far: 97.89%\n",
            "Batch 400/782 | Loss: 0.5121 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.203s | Batch: 0.475s | Acc so far: 97.85%\n",
            "Batch 450/782 | Loss: 0.6246 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.203s | Batch: 0.474s | Acc so far: 97.84%\n",
            "Batch 500/782 | Loss: 0.5222 | Data: 0.013s | Fwd: 0.018s | Bwd: 0.199s | Batch: 0.469s | Acc so far: 97.83%\n",
            "Batch 550/782 | Loss: 0.5768 | Data: 0.011s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.468s | Acc so far: 97.82%\n",
            "Batch 600/782 | Loss: 0.5640 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.207s | Batch: 0.477s | Acc so far: 97.79%\n",
            "Batch 650/782 | Loss: 0.5880 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.467s | Acc so far: 97.75%\n",
            "Batch 700/782 | Loss: 0.5238 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.470s | Acc so far: 97.77%\n",
            "Batch 750/782 | Loss: 0.5064 | Data: 0.015s | Fwd: 0.022s | Bwd: 0.195s | Batch: 0.470s | Acc so far: 97.75%\n",
            "Epoch finished in 379.06s\n",
            "[Eval] Batch 50/157 | Loss: 0.6527 | Acc so far: 97.88%\n",
            "[Eval] Batch 100/157 | Loss: 0.5269 | Acc so far: 97.78%\n",
            "[Eval] Batch 150/157 | Loss: 0.5741 | Acc so far: 97.71%\n",
            "Evaluation finished in 106.59s\n",
            "Epoch 8/30 | Train Loss: 0.5533, Train Acc: 97.75% | Val Loss: 0.5587, Val Acc: 97.74%\n",
            "\n",
            "--- Epoch 9/30 ---\n",
            "Batch 50/782 | Loss: 0.5246 | Data: 0.023s | Fwd: 0.025s | Bwd: 0.188s | Batch: 0.475s | Acc so far: 98.06%\n",
            "Batch 100/782 | Loss: 0.5306 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.203s | Batch: 0.473s | Acc so far: 97.81%\n",
            "Batch 150/782 | Loss: 0.5682 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.471s | Acc so far: 97.77%\n",
            "Batch 200/782 | Loss: 0.5745 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.468s | Acc so far: 97.84%\n",
            "Batch 250/782 | Loss: 0.5067 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.468s | Acc so far: 97.83%\n",
            "Batch 300/782 | Loss: 0.5271 | Data: 0.012s | Fwd: 0.017s | Bwd: 0.199s | Batch: 0.471s | Acc so far: 97.83%\n",
            "Batch 350/782 | Loss: 0.5384 | Data: 0.015s | Fwd: 0.016s | Bwd: 0.203s | Batch: 0.476s | Acc so far: 97.83%\n",
            "Batch 400/782 | Loss: 0.5895 | Data: 0.012s | Fwd: 0.022s | Bwd: 0.194s | Batch: 0.471s | Acc so far: 97.81%\n",
            "Batch 450/782 | Loss: 0.5031 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.202s | Batch: 0.468s | Acc so far: 97.87%\n",
            "Batch 500/782 | Loss: 0.5098 | Data: 0.011s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.469s | Acc so far: 97.85%\n",
            "Batch 550/782 | Loss: 0.5466 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.471s | Acc so far: 97.85%\n",
            "Batch 600/782 | Loss: 0.5145 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.468s | Acc so far: 97.85%\n",
            "Batch 650/782 | Loss: 0.5247 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.198s | Batch: 0.465s | Acc so far: 97.83%\n",
            "Batch 700/782 | Loss: 0.5691 | Data: 0.014s | Fwd: 0.024s | Bwd: 0.189s | Batch: 0.465s | Acc so far: 97.79%\n",
            "Batch 750/782 | Loss: 0.5546 | Data: 0.028s | Fwd: 0.041s | Bwd: 0.176s | Batch: 0.489s | Acc so far: 97.80%\n",
            "Epoch finished in 377.97s\n",
            "[Eval] Batch 50/157 | Loss: 0.6468 | Acc so far: 97.06%\n",
            "[Eval] Batch 100/157 | Loss: 0.5687 | Acc so far: 97.39%\n",
            "[Eval] Batch 150/157 | Loss: 0.6020 | Acc so far: 97.53%\n",
            "Evaluation finished in 106.49s\n",
            "Epoch 9/30 | Train Loss: 0.5521, Train Acc: 97.78% | Val Loss: 0.5631, Val Acc: 97.59%\n",
            "\n",
            "--- Epoch 10/30 ---\n",
            "Batch 50/782 | Loss: 0.5950 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.203s | Batch: 0.473s | Acc so far: 97.50%\n",
            "Batch 100/782 | Loss: 0.5604 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.469s | Acc so far: 97.36%\n",
            "Batch 150/782 | Loss: 0.5714 | Data: 0.012s | Fwd: 0.017s | Bwd: 0.202s | Batch: 0.474s | Acc so far: 97.69%\n",
            "Batch 200/782 | Loss: 0.6183 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.203s | Batch: 0.474s | Acc so far: 97.66%\n",
            "Batch 250/782 | Loss: 0.5339 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.468s | Acc so far: 97.74%\n",
            "Batch 300/782 | Loss: 0.5139 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.469s | Acc so far: 97.81%\n",
            "Batch 350/782 | Loss: 0.5396 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.204s | Batch: 0.473s | Acc so far: 97.83%\n",
            "Batch 400/782 | Loss: 0.5669 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.202s | Batch: 0.471s | Acc so far: 97.78%\n",
            "Batch 450/782 | Loss: 0.6406 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.201s | Batch: 0.467s | Acc so far: 97.80%\n",
            "Batch 500/782 | Loss: 0.5680 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.470s | Acc so far: 97.82%\n",
            "Batch 550/782 | Loss: 0.5486 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.203s | Batch: 0.473s | Acc so far: 97.85%\n",
            "Batch 600/782 | Loss: 0.5120 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.466s | Acc so far: 97.86%\n",
            "Batch 650/782 | Loss: 0.5798 | Data: 0.015s | Fwd: 0.020s | Bwd: 0.193s | Batch: 0.471s | Acc so far: 97.88%\n",
            "Batch 700/782 | Loss: 0.5776 | Data: 0.023s | Fwd: 0.027s | Bwd: 0.190s | Batch: 0.477s | Acc so far: 97.88%\n",
            "Batch 750/782 | Loss: 0.5102 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.465s | Acc so far: 97.89%\n",
            "Epoch finished in 378.32s\n",
            "[Eval] Batch 50/157 | Loss: 0.6311 | Acc so far: 97.62%\n",
            "[Eval] Batch 100/157 | Loss: 0.5526 | Acc so far: 97.69%\n",
            "[Eval] Batch 150/157 | Loss: 0.5665 | Acc so far: 97.72%\n",
            "Evaluation finished in 106.42s\n",
            "Epoch 10/30 | Train Loss: 0.5491, Train Acc: 97.91% | Val Loss: 0.5561, Val Acc: 97.73%\n",
            "\n",
            "--- Epoch 11/30 ---\n",
            "Batch 50/782 | Loss: 0.5732 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.469s | Acc so far: 98.06%\n",
            "Batch 100/782 | Loss: 0.5093 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.204s | Batch: 0.475s | Acc so far: 98.30%\n",
            "Batch 150/782 | Loss: 0.5331 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.469s | Acc so far: 98.34%\n",
            "Batch 200/782 | Loss: 0.6062 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.468s | Acc so far: 98.35%\n",
            "Batch 250/782 | Loss: 0.5146 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.202s | Batch: 0.469s | Acc so far: 98.33%\n",
            "Batch 300/782 | Loss: 0.5200 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.467s | Acc so far: 98.32%\n",
            "Batch 350/782 | Loss: 0.6360 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.472s | Acc so far: 98.26%\n",
            "Batch 400/782 | Loss: 0.5458 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.468s | Acc so far: 98.20%\n",
            "Batch 450/782 | Loss: 0.6848 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.468s | Acc so far: 98.18%\n",
            "Batch 500/782 | Loss: 0.6499 | Data: 0.022s | Fwd: 0.014s | Bwd: 0.202s | Batch: 0.479s | Acc so far: 98.21%\n",
            "Batch 550/782 | Loss: 0.5431 | Data: 0.024s | Fwd: 0.039s | Bwd: 0.178s | Batch: 0.483s | Acc so far: 98.22%\n",
            "Batch 600/782 | Loss: 0.6453 | Data: 0.022s | Fwd: 0.045s | Bwd: 0.169s | Batch: 0.481s | Acc so far: 98.18%\n",
            "Batch 650/782 | Loss: 0.6200 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.203s | Batch: 0.468s | Acc so far: 98.10%\n",
            "Batch 700/782 | Loss: 0.5578 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.462s | Acc so far: 98.12%\n",
            "Batch 750/782 | Loss: 0.5441 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.474s | Acc so far: 98.10%\n",
            "Epoch finished in 377.52s\n",
            "[Eval] Batch 50/157 | Loss: 0.5995 | Acc so far: 97.72%\n",
            "[Eval] Batch 100/157 | Loss: 0.5637 | Acc so far: 97.53%\n",
            "[Eval] Batch 150/157 | Loss: 0.5526 | Acc so far: 97.65%\n",
            "Evaluation finished in 106.57s\n",
            "Epoch 11/30 | Train Loss: 0.5465, Train Acc: 98.10% | Val Loss: 0.5579, Val Acc: 97.64%\n",
            "\n",
            "--- Epoch 12/30 ---\n",
            "Batch 50/782 | Loss: 0.5670 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.468s | Acc so far: 98.12%\n",
            "Batch 100/782 | Loss: 0.5472 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.466s | Acc so far: 98.25%\n",
            "Batch 150/782 | Loss: 0.5549 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.466s | Acc so far: 98.30%\n",
            "Batch 200/782 | Loss: 0.5317 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.205s | Batch: 0.472s | Acc so far: 98.30%\n",
            "Batch 250/782 | Loss: 0.5040 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.466s | Acc so far: 98.42%\n",
            "Batch 300/782 | Loss: 0.5144 | Data: 0.013s | Fwd: 0.016s | Bwd: 0.197s | Batch: 0.465s | Acc so far: 98.40%\n",
            "Batch 350/782 | Loss: 0.5153 | Data: 0.023s | Fwd: 0.031s | Bwd: 0.184s | Batch: 0.479s | Acc so far: 98.38%\n",
            "Batch 400/782 | Loss: 0.5650 | Data: 0.021s | Fwd: 0.025s | Bwd: 0.188s | Batch: 0.477s | Acc so far: 98.35%\n",
            "Batch 450/782 | Loss: 0.5055 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.203s | Batch: 0.474s | Acc so far: 98.32%\n",
            "Batch 500/782 | Loss: 0.5481 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.466s | Acc so far: 98.27%\n",
            "Batch 550/782 | Loss: 0.6008 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.469s | Acc so far: 98.29%\n",
            "Batch 600/782 | Loss: 0.5488 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.209s | Batch: 0.479s | Acc so far: 98.27%\n",
            "Batch 650/782 | Loss: 0.5342 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.472s | Acc so far: 98.26%\n",
            "Batch 700/782 | Loss: 0.5899 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.197s | Batch: 0.463s | Acc so far: 98.25%\n",
            "Batch 750/782 | Loss: 0.5071 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.467s | Acc so far: 98.25%\n",
            "Epoch finished in 376.81s\n",
            "[Eval] Batch 50/157 | Loss: 0.6231 | Acc so far: 97.84%\n",
            "[Eval] Batch 100/157 | Loss: 0.5741 | Acc so far: 97.77%\n",
            "[Eval] Batch 150/157 | Loss: 0.5664 | Acc so far: 97.83%\n",
            "Evaluation finished in 106.42s\n",
            "Epoch 12/30 | Train Loss: 0.5421, Train Acc: 98.25% | Val Loss: 0.5552, Val Acc: 97.85%\n",
            "\n",
            "--- Epoch 13/30 ---\n",
            "Batch 50/782 | Loss: 0.5129 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.469s | Acc so far: 98.62%\n",
            "Batch 100/782 | Loss: 0.5566 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.469s | Acc so far: 98.59%\n",
            "Batch 150/782 | Loss: 0.5053 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.467s | Acc so far: 98.59%\n",
            "Batch 200/782 | Loss: 0.5239 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.471s | Acc so far: 98.56%\n",
            "Batch 250/782 | Loss: 0.5408 | Data: 0.024s | Fwd: 0.032s | Bwd: 0.185s | Batch: 0.481s | Acc so far: 98.53%\n",
            "Batch 300/782 | Loss: 0.5406 | Data: 0.011s | Fwd: 0.013s | Bwd: 0.202s | Batch: 0.511s | Acc so far: 98.53%\n",
            "Batch 350/782 | Loss: 0.5015 | Data: 0.012s | Fwd: 0.019s | Bwd: 0.199s | Batch: 0.469s | Acc so far: 98.54%\n",
            "Batch 400/782 | Loss: 0.5993 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.469s | Acc so far: 98.55%\n",
            "Batch 450/782 | Loss: 0.5028 | Data: 0.013s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.467s | Acc so far: 98.55%\n",
            "Batch 500/782 | Loss: 0.5484 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.470s | Acc so far: 98.52%\n",
            "Batch 550/782 | Loss: 0.5073 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.468s | Acc so far: 98.47%\n",
            "Batch 600/782 | Loss: 0.5390 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.472s | Acc so far: 98.45%\n",
            "Batch 650/782 | Loss: 0.5150 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.464s | Acc so far: 98.40%\n",
            "Batch 700/782 | Loss: 0.5327 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.204s | Batch: 0.473s | Acc so far: 98.37%\n",
            "Batch 750/782 | Loss: 0.5712 | Data: 0.013s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.466s | Acc so far: 98.36%\n",
            "Epoch finished in 376.73s\n",
            "[Eval] Batch 50/157 | Loss: 0.6069 | Acc so far: 98.34%\n",
            "[Eval] Batch 100/157 | Loss: 0.5650 | Acc so far: 98.19%\n",
            "[Eval] Batch 150/157 | Loss: 0.5369 | Acc so far: 98.08%\n",
            "Evaluation finished in 106.40s\n",
            "Epoch 13/30 | Train Loss: 0.5397, Train Acc: 98.36% | Val Loss: 0.5487, Val Acc: 98.12%\n",
            "\n",
            "--- Epoch 14/30 ---\n",
            "Batch 50/782 | Loss: 0.6218 | Data: 0.013s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.471s | Acc so far: 97.88%\n",
            "Batch 100/782 | Loss: 0.5348 | Data: 0.026s | Fwd: 0.037s | Bwd: 0.178s | Batch: 0.482s | Acc so far: 98.02%\n",
            "Batch 150/782 | Loss: 0.5609 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.199s | Batch: 0.463s | Acc so far: 98.03%\n",
            "Batch 200/782 | Loss: 0.5393 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.467s | Acc so far: 98.11%\n",
            "Batch 250/782 | Loss: 0.5381 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.466s | Acc so far: 98.14%\n",
            "Batch 300/782 | Loss: 0.5561 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.466s | Acc so far: 98.15%\n",
            "Batch 350/782 | Loss: 0.5237 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.468s | Acc so far: 98.22%\n",
            "Batch 400/782 | Loss: 0.5055 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.466s | Acc so far: 98.27%\n",
            "Batch 450/782 | Loss: 0.5064 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.468s | Acc so far: 98.27%\n",
            "Batch 500/782 | Loss: 0.5733 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.469s | Acc so far: 98.30%\n",
            "Batch 550/782 | Loss: 0.5725 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.465s | Acc so far: 98.31%\n",
            "Batch 600/782 | Loss: 0.5298 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.200s | Batch: 0.468s | Acc so far: 98.32%\n",
            "Batch 650/782 | Loss: 0.5255 | Data: 0.011s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.468s | Acc so far: 98.34%\n",
            "Batch 700/782 | Loss: 0.6019 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.198s | Batch: 0.464s | Acc so far: 98.37%\n",
            "Batch 750/782 | Loss: 0.5090 | Data: 0.021s | Fwd: 0.027s | Bwd: 0.188s | Batch: 0.477s | Acc so far: 98.36%\n",
            "Epoch finished in 376.21s\n",
            "[Eval] Batch 50/157 | Loss: 0.5802 | Acc so far: 98.06%\n",
            "[Eval] Batch 100/157 | Loss: 0.5448 | Acc so far: 98.03%\n",
            "[Eval] Batch 150/157 | Loss: 0.6296 | Acc so far: 98.06%\n",
            "Evaluation finished in 106.42s\n",
            "Epoch 14/30 | Train Loss: 0.5382, Train Acc: 98.36% | Val Loss: 0.5492, Val Acc: 98.07%\n",
            "\n",
            "--- Epoch 15/30 ---\n",
            "Batch 50/782 | Loss: 0.5141 | Data: 0.021s | Fwd: 0.030s | Bwd: 0.185s | Batch: 0.474s | Acc so far: 98.28%\n",
            "Batch 100/782 | Loss: 0.5175 | Data: 0.011s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.469s | Acc so far: 98.42%\n",
            "Batch 150/782 | Loss: 0.5259 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.466s | Acc so far: 98.53%\n",
            "Batch 200/782 | Loss: 0.5236 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.467s | Acc so far: 98.67%\n",
            "Batch 250/782 | Loss: 0.5321 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.466s | Acc so far: 98.56%\n",
            "Batch 300/782 | Loss: 0.5377 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.466s | Acc so far: 98.57%\n",
            "Batch 350/782 | Loss: 0.5324 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.467s | Acc so far: 98.57%\n",
            "Batch 400/782 | Loss: 0.5187 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.465s | Acc so far: 98.60%\n",
            "Batch 450/782 | Loss: 0.5521 | Data: 0.020s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.477s | Acc so far: 98.57%\n",
            "Batch 500/782 | Loss: 0.5051 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.464s | Acc so far: 98.56%\n",
            "Batch 550/782 | Loss: 0.5733 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.468s | Acc so far: 98.55%\n",
            "Batch 600/782 | Loss: 0.5341 | Data: 0.011s | Fwd: 0.019s | Bwd: 0.202s | Batch: 0.474s | Acc so far: 98.52%\n",
            "Batch 650/782 | Loss: 0.5022 | Data: 0.013s | Fwd: 0.026s | Bwd: 0.188s | Batch: 0.468s | Acc so far: 98.51%\n",
            "Batch 700/782 | Loss: 0.5032 | Data: 0.011s | Fwd: 0.015s | Bwd: 0.204s | Batch: 0.471s | Acc so far: 98.51%\n",
            "Batch 750/782 | Loss: 0.5058 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.468s | Acc so far: 98.53%\n",
            "Epoch finished in 376.53s\n",
            "[Eval] Batch 50/157 | Loss: 0.5826 | Acc so far: 98.09%\n",
            "[Eval] Batch 100/157 | Loss: 0.5456 | Acc so far: 98.05%\n",
            "[Eval] Batch 150/157 | Loss: 0.6063 | Acc so far: 98.14%\n",
            "Evaluation finished in 106.46s\n",
            "Epoch 15/30 | Train Loss: 0.5353, Train Acc: 98.52% | Val Loss: 0.5508, Val Acc: 98.12%\n",
            "\n",
            "--- Epoch 16/30 ---\n",
            "Batch 50/782 | Loss: 0.5310 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.464s | Acc so far: 98.59%\n",
            "Batch 100/782 | Loss: 0.5721 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.203s | Batch: 0.470s | Acc so far: 98.78%\n",
            "Batch 150/782 | Loss: 0.5105 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.464s | Acc so far: 98.77%\n",
            "Batch 200/782 | Loss: 0.5513 | Data: 0.013s | Fwd: 0.017s | Bwd: 0.196s | Batch: 0.465s | Acc so far: 98.66%\n",
            "Batch 250/782 | Loss: 0.5789 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.471s | Acc so far: 98.58%\n",
            "Batch 300/782 | Loss: 0.5088 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.195s | Batch: 0.463s | Acc so far: 98.62%\n",
            "Batch 350/782 | Loss: 0.5061 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.200s | Batch: 0.467s | Acc so far: 98.65%\n",
            "Batch 400/782 | Loss: 0.5852 | Data: 0.020s | Fwd: 0.028s | Bwd: 0.190s | Batch: 0.475s | Acc so far: 98.65%\n",
            "Batch 450/782 | Loss: 0.5784 | Data: 0.022s | Fwd: 0.034s | Bwd: 0.183s | Batch: 0.477s | Acc so far: 98.67%\n",
            "Batch 500/782 | Loss: 0.5411 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.469s | Acc so far: 98.67%\n",
            "Batch 550/782 | Loss: 0.5464 | Data: 0.012s | Fwd: 0.018s | Bwd: 0.199s | Batch: 0.467s | Acc so far: 98.65%\n",
            "Batch 600/782 | Loss: 0.5148 | Data: 0.013s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.467s | Acc so far: 98.62%\n",
            "Batch 650/782 | Loss: 0.5503 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.204s | Batch: 0.472s | Acc so far: 98.63%\n",
            "Batch 700/782 | Loss: 0.5021 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.196s | Batch: 0.465s | Acc so far: 98.62%\n",
            "Batch 750/782 | Loss: 0.5356 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.467s | Acc so far: 98.62%\n",
            "Epoch finished in 376.05s\n",
            "[Eval] Batch 50/157 | Loss: 0.5534 | Acc so far: 97.84%\n",
            "[Eval] Batch 100/157 | Loss: 0.5268 | Acc so far: 97.83%\n",
            "[Eval] Batch 150/157 | Loss: 0.5945 | Acc so far: 97.88%\n",
            "Evaluation finished in 106.53s\n",
            "Epoch 16/30 | Train Loss: 0.5333, Train Acc: 98.62% | Val Loss: 0.5534, Val Acc: 97.91%\n",
            "\n",
            "--- Epoch 17/30 ---\n",
            "Batch 50/782 | Loss: 0.5658 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.468s | Acc so far: 98.56%\n",
            "Batch 100/782 | Loss: 0.5084 | Data: 0.011s | Fwd: 0.014s | Bwd: 0.202s | Batch: 0.465s | Acc so far: 98.52%\n",
            "Batch 150/782 | Loss: 0.5623 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.465s | Acc so far: 98.54%\n",
            "Batch 200/782 | Loss: 0.5210 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.467s | Acc so far: 98.61%\n",
            "Batch 250/782 | Loss: 0.5857 | Data: 0.012s | Fwd: 0.017s | Bwd: 0.197s | Batch: 0.468s | Acc so far: 98.65%\n",
            "Batch 300/782 | Loss: 0.5012 | Data: 0.023s | Fwd: 0.025s | Bwd: 0.189s | Batch: 0.477s | Acc so far: 98.67%\n",
            "Batch 350/782 | Loss: 0.5357 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.471s | Acc so far: 98.69%\n",
            "Batch 400/782 | Loss: 0.5361 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.465s | Acc so far: 98.68%\n",
            "Batch 450/782 | Loss: 0.5576 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.468s | Acc so far: 98.69%\n",
            "Batch 500/782 | Loss: 0.5338 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.464s | Acc so far: 98.72%\n",
            "Batch 550/782 | Loss: 0.5257 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.469s | Acc so far: 98.72%\n",
            "Batch 600/782 | Loss: 0.5392 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.468s | Acc so far: 98.67%\n",
            "Batch 650/782 | Loss: 0.5437 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.202s | Batch: 0.467s | Acc so far: 98.68%\n",
            "Batch 700/782 | Loss: 0.5842 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.202s | Batch: 0.469s | Acc so far: 98.67%\n",
            "Batch 750/782 | Loss: 0.5312 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.205s | Batch: 0.474s | Acc so far: 98.68%\n",
            "Epoch finished in 375.93s\n",
            "[Eval] Batch 50/157 | Loss: 0.5923 | Acc so far: 98.31%\n",
            "[Eval] Batch 100/157 | Loss: 0.5046 | Acc so far: 98.38%\n",
            "[Eval] Batch 150/157 | Loss: 0.6305 | Acc so far: 98.43%\n",
            "Evaluation finished in 106.54s\n",
            "Epoch 17/30 | Train Loss: 0.5308, Train Acc: 98.67% | Val Loss: 0.5422, Val Acc: 98.44%\n",
            "\n",
            "--- Epoch 18/30 ---\n",
            "Batch 50/782 | Loss: 0.5016 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.462s | Acc so far: 98.88%\n",
            "Batch 100/782 | Loss: 0.5625 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.200s | Batch: 0.467s | Acc so far: 98.92%\n",
            "Batch 150/782 | Loss: 0.5088 | Data: 0.024s | Fwd: 0.034s | Bwd: 0.182s | Batch: 0.479s | Acc so far: 98.85%\n",
            "Batch 200/782 | Loss: 0.6130 | Data: 0.023s | Fwd: 0.028s | Bwd: 0.191s | Batch: 0.479s | Acc so far: 98.82%\n",
            "Batch 250/782 | Loss: 0.5043 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.464s | Acc so far: 98.76%\n",
            "Batch 300/782 | Loss: 0.5052 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.470s | Acc so far: 98.81%\n",
            "Batch 350/782 | Loss: 0.5039 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.463s | Acc so far: 98.83%\n",
            "Batch 400/782 | Loss: 0.5049 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.465s | Acc so far: 98.75%\n",
            "Batch 450/782 | Loss: 0.5434 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.467s | Acc so far: 98.74%\n",
            "Batch 500/782 | Loss: 0.5459 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.465s | Acc so far: 98.75%\n",
            "Batch 550/782 | Loss: 0.5250 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.468s | Acc so far: 98.72%\n",
            "Batch 600/782 | Loss: 0.5204 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.466s | Acc so far: 98.71%\n",
            "Batch 650/782 | Loss: 0.5244 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.466s | Acc so far: 98.74%\n",
            "Batch 700/782 | Loss: 0.5275 | Data: 0.015s | Fwd: 0.017s | Bwd: 0.196s | Batch: 0.466s | Acc so far: 98.74%\n",
            "Batch 750/782 | Loss: 0.5168 | Data: 0.011s | Fwd: 0.013s | Bwd: 0.204s | Batch: 0.467s | Acc so far: 98.76%\n",
            "Epoch finished in 375.07s\n",
            "[Eval] Batch 50/157 | Loss: 0.5368 | Acc so far: 98.22%\n",
            "[Eval] Batch 100/157 | Loss: 0.5544 | Acc so far: 98.11%\n",
            "[Eval] Batch 150/157 | Loss: 0.5353 | Acc so far: 98.20%\n",
            "Evaluation finished in 106.40s\n",
            "Epoch 18/30 | Train Loss: 0.5292, Train Acc: 98.76% | Val Loss: 0.5459, Val Acc: 98.24%\n",
            "\n",
            "--- Epoch 19/30 ---\n",
            "Batch 50/782 | Loss: 0.5156 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.465s | Acc so far: 98.81%\n",
            "Batch 100/782 | Loss: 0.5032 | Data: 0.012s | Fwd: 0.017s | Bwd: 0.201s | Batch: 0.468s | Acc so far: 98.75%\n",
            "Batch 150/782 | Loss: 0.5389 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.203s | Batch: 0.468s | Acc so far: 98.86%\n",
            "Batch 200/782 | Loss: 0.5132 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.466s | Acc so far: 98.88%\n",
            "Batch 250/782 | Loss: 0.5413 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.198s | Batch: 0.463s | Acc so far: 98.90%\n",
            "Batch 300/782 | Loss: 0.5348 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.469s | Acc so far: 98.91%\n",
            "Batch 350/782 | Loss: 0.5062 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.463s | Acc so far: 98.90%\n",
            "Batch 400/782 | Loss: 0.6462 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.464s | Acc so far: 98.88%\n",
            "Batch 450/782 | Loss: 0.5407 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.466s | Acc so far: 98.89%\n",
            "Batch 500/782 | Loss: 0.5786 | Data: 0.026s | Fwd: 0.042s | Bwd: 0.171s | Batch: 0.480s | Acc so far: 98.90%\n",
            "Batch 550/782 | Loss: 0.5107 | Data: 0.023s | Fwd: 0.028s | Bwd: 0.187s | Batch: 0.478s | Acc so far: 98.89%\n",
            "Batch 600/782 | Loss: 0.5197 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.466s | Acc so far: 98.88%\n",
            "Batch 650/782 | Loss: 0.5831 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.470s | Acc so far: 98.88%\n",
            "Batch 700/782 | Loss: 0.5165 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.469s | Acc so far: 98.88%\n",
            "Batch 750/782 | Loss: 0.5083 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.465s | Acc so far: 98.88%\n",
            "Epoch finished in 375.32s\n",
            "[Eval] Batch 50/157 | Loss: 0.5471 | Acc so far: 98.25%\n",
            "[Eval] Batch 100/157 | Loss: 0.5621 | Acc so far: 98.34%\n",
            "[Eval] Batch 150/157 | Loss: 0.5189 | Acc so far: 98.41%\n",
            "Evaluation finished in 106.39s\n",
            "Epoch 19/30 | Train Loss: 0.5261, Train Acc: 98.88% | Val Loss: 0.5413, Val Acc: 98.43%\n",
            "\n",
            "--- Epoch 20/30 ---\n",
            "Batch 50/782 | Loss: 0.5278 | Data: 0.013s | Fwd: 0.018s | Bwd: 0.197s | Batch: 0.467s | Acc so far: 98.75%\n",
            "Batch 100/782 | Loss: 0.6174 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.469s | Acc so far: 98.80%\n",
            "Batch 150/782 | Loss: 0.5011 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.466s | Acc so far: 98.90%\n",
            "Batch 200/782 | Loss: 0.5013 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.462s | Acc so far: 98.91%\n",
            "Batch 250/782 | Loss: 0.5032 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.462s | Acc so far: 98.89%\n",
            "Batch 300/782 | Loss: 0.5405 | Data: 0.024s | Fwd: 0.037s | Bwd: 0.179s | Batch: 0.481s | Acc so far: 98.88%\n",
            "Batch 350/782 | Loss: 0.5200 | Data: 0.021s | Fwd: 0.028s | Bwd: 0.187s | Batch: 0.478s | Acc so far: 98.89%\n",
            "Batch 400/782 | Loss: 0.5106 | Data: 0.014s | Fwd: 0.021s | Bwd: 0.193s | Batch: 0.494s | Acc so far: 98.93%\n",
            "Batch 450/782 | Loss: 0.5223 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.469s | Acc so far: 98.95%\n",
            "Batch 500/782 | Loss: 0.5299 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.203s | Batch: 0.467s | Acc so far: 98.95%\n",
            "Batch 550/782 | Loss: 0.5756 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.466s | Acc so far: 98.92%\n",
            "Batch 600/782 | Loss: 0.5072 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.200s | Batch: 0.466s | Acc so far: 98.91%\n",
            "Batch 650/782 | Loss: 0.5218 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.463s | Acc so far: 98.93%\n",
            "Batch 700/782 | Loss: 0.5139 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.200s | Batch: 0.465s | Acc so far: 98.92%\n",
            "Batch 750/782 | Loss: 0.5738 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.207s | Batch: 0.471s | Acc so far: 98.90%\n",
            "Epoch finished in 375.04s\n",
            "[Eval] Batch 50/157 | Loss: 0.5092 | Acc so far: 98.16%\n",
            "[Eval] Batch 100/157 | Loss: 0.6199 | Acc so far: 98.25%\n",
            "[Eval] Batch 150/157 | Loss: 0.5622 | Acc so far: 98.30%\n",
            "Evaluation finished in 106.24s\n",
            "Epoch 20/30 | Train Loss: 0.5258, Train Acc: 98.89% | Val Loss: 0.5442, Val Acc: 98.33%\n",
            "\n",
            "--- Epoch 21/30 ---\n",
            "Batch 50/782 | Loss: 0.5179 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.466s | Acc so far: 98.84%\n",
            "Batch 100/782 | Loss: 0.5010 | Data: 0.013s | Fwd: 0.021s | Bwd: 0.196s | Batch: 0.467s | Acc so far: 98.80%\n",
            "Batch 150/782 | Loss: 0.5030 | Data: 0.022s | Fwd: 0.024s | Bwd: 0.191s | Batch: 0.479s | Acc so far: 98.98%\n",
            "Batch 200/782 | Loss: 0.5013 | Data: 0.011s | Fwd: 0.018s | Bwd: 0.198s | Batch: 0.466s | Acc so far: 98.94%\n",
            "Batch 250/782 | Loss: 0.5065 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.463s | Acc so far: 98.93%\n",
            "Batch 300/782 | Loss: 0.5137 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.464s | Acc so far: 98.97%\n",
            "Batch 350/782 | Loss: 0.5371 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.467s | Acc so far: 98.98%\n",
            "Batch 400/782 | Loss: 0.5017 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.465s | Acc so far: 99.02%\n",
            "Batch 450/782 | Loss: 0.5021 | Data: 0.013s | Fwd: 0.016s | Bwd: 0.197s | Batch: 0.465s | Acc so far: 98.95%\n",
            "Batch 500/782 | Loss: 0.5381 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.465s | Acc so far: 98.97%\n",
            "Batch 550/782 | Loss: 0.5437 | Data: 0.011s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.463s | Acc so far: 98.96%\n",
            "Batch 600/782 | Loss: 0.5038 | Data: 0.011s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.467s | Acc so far: 98.97%\n",
            "Batch 650/782 | Loss: 0.5096 | Data: 0.011s | Fwd: 0.014s | Bwd: 0.200s | Batch: 0.466s | Acc so far: 98.97%\n",
            "Batch 700/782 | Loss: 0.5187 | Data: 0.030s | Fwd: 0.021s | Bwd: 0.189s | Batch: 0.481s | Acc so far: 98.97%\n",
            "Batch 750/782 | Loss: 0.5015 | Data: 0.023s | Fwd: 0.027s | Bwd: 0.186s | Batch: 0.474s | Acc so far: 98.98%\n",
            "Epoch finished in 374.82s\n",
            "[Eval] Batch 50/157 | Loss: 0.5380 | Acc so far: 98.19%\n",
            "[Eval] Batch 100/157 | Loss: 0.6067 | Acc so far: 98.34%\n",
            "[Eval] Batch 150/157 | Loss: 0.5512 | Acc so far: 98.44%\n",
            "Evaluation finished in 106.45s\n",
            "Epoch 21/30 | Train Loss: 0.5243, Train Acc: 98.97% | Val Loss: 0.5425, Val Acc: 98.46%\n",
            "\n",
            "--- Epoch 22/30 ---\n",
            "Batch 50/782 | Loss: 0.5129 | Data: 0.012s | Fwd: 0.018s | Bwd: 0.197s | Batch: 0.465s | Acc so far: 98.94%\n",
            "Batch 100/782 | Loss: 0.5630 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.466s | Acc so far: 98.84%\n",
            "Batch 150/782 | Loss: 0.5089 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.468s | Acc so far: 98.90%\n",
            "Batch 200/782 | Loss: 0.5020 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.465s | Acc so far: 98.91%\n",
            "Batch 250/782 | Loss: 0.5109 | Data: 0.011s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.469s | Acc so far: 98.94%\n",
            "Batch 300/782 | Loss: 0.5199 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.466s | Acc so far: 98.95%\n",
            "Batch 350/782 | Loss: 0.5481 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.465s | Acc so far: 98.92%\n",
            "Batch 400/782 | Loss: 0.5636 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.465s | Acc so far: 98.94%\n",
            "Batch 450/782 | Loss: 0.5122 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.467s | Acc so far: 98.96%\n",
            "Batch 500/782 | Loss: 0.5306 | Data: 0.013s | Fwd: 0.018s | Bwd: 0.198s | Batch: 0.464s | Acc so far: 98.94%\n",
            "Batch 550/782 | Loss: 0.5123 | Data: 0.024s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.479s | Acc so far: 98.96%\n",
            "Batch 600/782 | Loss: 0.5250 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.203s | Batch: 0.466s | Acc so far: 98.95%\n",
            "Batch 650/782 | Loss: 0.5832 | Data: 0.011s | Fwd: 0.017s | Bwd: 0.201s | Batch: 0.471s | Acc so far: 98.95%\n",
            "Batch 700/782 | Loss: 0.5749 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.467s | Acc so far: 98.96%\n",
            "Batch 750/782 | Loss: 0.5014 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.464s | Acc so far: 98.96%\n",
            "Epoch finished in 375.17s\n",
            "[Eval] Batch 50/157 | Loss: 0.5801 | Acc so far: 98.38%\n",
            "[Eval] Batch 100/157 | Loss: 0.6009 | Acc so far: 98.45%\n",
            "[Eval] Batch 150/157 | Loss: 0.5605 | Acc so far: 98.50%\n",
            "Evaluation finished in 106.47s\n",
            "Epoch 22/30 | Train Loss: 0.5236, Train Acc: 98.97% | Val Loss: 0.5394, Val Acc: 98.53%\n",
            "\n",
            "--- Epoch 23/30 ---\n",
            "Batch 50/782 | Loss: 0.5012 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.198s | Batch: 0.461s | Acc so far: 99.09%\n",
            "Batch 100/782 | Loss: 0.5733 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.465s | Acc so far: 98.95%\n",
            "Batch 150/782 | Loss: 0.5186 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.205s | Batch: 0.472s | Acc so far: 99.01%\n",
            "Batch 200/782 | Loss: 0.5177 | Data: 0.013s | Fwd: 0.015s | Bwd: 0.198s | Batch: 0.466s | Acc so far: 99.02%\n",
            "Batch 250/782 | Loss: 0.5120 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.464s | Acc so far: 98.97%\n",
            "Batch 300/782 | Loss: 0.5008 | Data: 0.014s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.464s | Acc so far: 98.93%\n",
            "Batch 350/782 | Loss: 0.5575 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.197s | Batch: 0.463s | Acc so far: 98.96%\n",
            "Batch 400/782 | Loss: 0.5023 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.196s | Batch: 0.462s | Acc so far: 99.00%\n",
            "Batch 450/782 | Loss: 0.5162 | Data: 0.013s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.469s | Acc so far: 99.00%\n",
            "Batch 500/782 | Loss: 0.6316 | Data: 0.024s | Fwd: 0.032s | Bwd: 0.182s | Batch: 0.479s | Acc so far: 98.98%\n",
            "Batch 550/782 | Loss: 0.5693 | Data: 0.030s | Fwd: 0.029s | Bwd: 0.187s | Batch: 0.483s | Acc so far: 98.99%\n",
            "Batch 600/782 | Loss: 0.5037 | Data: 0.023s | Fwd: 0.030s | Bwd: 0.184s | Batch: 0.475s | Acc so far: 99.02%\n",
            "Batch 650/782 | Loss: 0.5500 | Data: 0.012s | Fwd: 0.013s | Bwd: 0.204s | Batch: 0.465s | Acc so far: 99.01%\n",
            "Batch 700/782 | Loss: 0.5244 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.468s | Acc so far: 99.02%\n",
            "Batch 750/782 | Loss: 0.5057 | Data: 0.013s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.464s | Acc so far: 99.03%\n",
            "Epoch finished in 375.32s\n",
            "[Eval] Batch 50/157 | Loss: 0.5080 | Acc so far: 98.47%\n",
            "[Eval] Batch 100/157 | Loss: 0.6054 | Acc so far: 98.55%\n",
            "[Eval] Batch 150/157 | Loss: 0.5264 | Acc so far: 98.64%\n",
            "Evaluation finished in 106.59s\n",
            "Epoch 23/30 | Train Loss: 0.5226, Train Acc: 99.03% | Val Loss: 0.5372, Val Acc: 98.66%\n",
            "\n",
            "--- Epoch 24/30 ---\n",
            "Batch 50/782 | Loss: 0.5112 | Data: 0.023s | Fwd: 0.029s | Bwd: 0.188s | Batch: 0.479s | Acc so far: 99.03%\n",
            "Batch 100/782 | Loss: 0.5115 | Data: 0.023s | Fwd: 0.028s | Bwd: 0.189s | Batch: 0.478s | Acc so far: 98.97%\n",
            "Batch 150/782 | Loss: 0.5104 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.466s | Acc so far: 99.03%\n",
            "Batch 200/782 | Loss: 0.5943 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.204s | Batch: 0.468s | Acc so far: 99.06%\n",
            "Batch 250/782 | Loss: 0.5028 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.202s | Batch: 0.471s | Acc so far: 99.11%\n",
            "Batch 300/782 | Loss: 0.5010 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.465s | Acc so far: 99.04%\n",
            "Batch 350/782 | Loss: 0.5343 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.198s | Batch: 0.465s | Acc so far: 99.03%\n",
            "Batch 400/782 | Loss: 0.5196 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.199s | Batch: 0.464s | Acc so far: 99.03%\n",
            "Batch 450/782 | Loss: 0.5083 | Data: 0.023s | Fwd: 0.024s | Bwd: 0.191s | Batch: 0.476s | Acc so far: 99.07%\n",
            "Batch 500/782 | Loss: 0.5007 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.459s | Acc so far: 99.10%\n",
            "Batch 550/782 | Loss: 0.5393 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.462s | Acc so far: 99.11%\n",
            "Batch 600/782 | Loss: 0.5075 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.459s | Acc so far: 99.12%\n",
            "Batch 650/782 | Loss: 0.5009 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.466s | Acc so far: 99.12%\n",
            "Batch 700/782 | Loss: 0.5010 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.466s | Acc so far: 99.12%\n",
            "Batch 750/782 | Loss: 0.5197 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.469s | Acc so far: 99.13%\n",
            "Epoch finished in 374.98s\n",
            "[Eval] Batch 50/157 | Loss: 0.5428 | Acc so far: 98.56%\n",
            "[Eval] Batch 100/157 | Loss: 0.5987 | Acc so far: 98.66%\n",
            "[Eval] Batch 150/157 | Loss: 0.5492 | Acc so far: 98.72%\n",
            "Evaluation finished in 106.64s\n",
            "Epoch 24/30 | Train Loss: 0.5209, Train Acc: 99.12% | Val Loss: 0.5348, Val Acc: 98.75%\n",
            "\n",
            "--- Epoch 25/30 ---\n",
            "Batch 50/782 | Loss: 0.5382 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.466s | Acc so far: 99.38%\n",
            "Batch 100/782 | Loss: 0.5012 | Data: 0.024s | Fwd: 0.031s | Bwd: 0.184s | Batch: 0.472s | Acc so far: 99.28%\n",
            "Batch 150/782 | Loss: 0.5008 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.204s | Batch: 0.469s | Acc so far: 99.21%\n",
            "Batch 200/782 | Loss: 0.5461 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.465s | Acc so far: 99.17%\n",
            "Batch 250/782 | Loss: 0.5266 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.464s | Acc so far: 99.17%\n",
            "Batch 300/782 | Loss: 0.5074 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.462s | Acc so far: 99.16%\n",
            "Batch 350/782 | Loss: 0.5746 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.463s | Acc so far: 99.17%\n",
            "Batch 400/782 | Loss: 0.5442 | Data: 0.013s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.467s | Acc so far: 99.14%\n",
            "Batch 450/782 | Loss: 0.5063 | Data: 0.013s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.469s | Acc so far: 99.12%\n",
            "Batch 500/782 | Loss: 0.5041 | Data: 0.024s | Fwd: 0.033s | Bwd: 0.181s | Batch: 0.480s | Acc so far: 99.14%\n",
            "Batch 550/782 | Loss: 0.5205 | Data: 0.032s | Fwd: 0.026s | Bwd: 0.189s | Batch: 0.483s | Acc so far: 99.15%\n",
            "Batch 600/782 | Loss: 0.5457 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.200s | Batch: 0.465s | Acc so far: 99.10%\n",
            "Batch 650/782 | Loss: 0.5010 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.464s | Acc so far: 99.11%\n",
            "Batch 700/782 | Loss: 0.5353 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.202s | Batch: 0.468s | Acc so far: 99.11%\n",
            "Batch 750/782 | Loss: 0.5197 | Data: 0.013s | Fwd: 0.016s | Bwd: 0.198s | Batch: 0.467s | Acc so far: 99.10%\n",
            "Epoch finished in 374.86s\n",
            "[Eval] Batch 50/157 | Loss: 0.5370 | Acc so far: 98.62%\n",
            "[Eval] Batch 100/157 | Loss: 0.6024 | Acc so far: 98.61%\n",
            "[Eval] Batch 150/157 | Loss: 0.5776 | Acc so far: 98.71%\n",
            "Evaluation finished in 106.64s\n",
            "Epoch 25/30 | Train Loss: 0.5209, Train Acc: 99.10% | Val Loss: 0.5349, Val Acc: 98.73%\n",
            "\n",
            "--- Epoch 26/30 ---\n",
            "Batch 50/782 | Loss: 0.5560 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.462s | Acc so far: 99.03%\n",
            "Batch 100/782 | Loss: 0.5375 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.201s | Batch: 0.467s | Acc so far: 99.19%\n",
            "Batch 150/782 | Loss: 0.5443 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.465s | Acc so far: 99.30%\n",
            "Batch 200/782 | Loss: 0.5215 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.468s | Acc so far: 99.32%\n",
            "Batch 250/782 | Loss: 0.5047 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.467s | Acc so far: 99.34%\n",
            "Batch 300/782 | Loss: 0.5024 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.198s | Batch: 0.463s | Acc so far: 99.29%\n",
            "Batch 350/782 | Loss: 0.5765 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.463s | Acc so far: 99.24%\n",
            "Batch 400/782 | Loss: 0.5042 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.466s | Acc so far: 99.23%\n",
            "Batch 450/782 | Loss: 0.5220 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.205s | Batch: 0.468s | Acc so far: 99.23%\n",
            "Batch 500/782 | Loss: 0.5018 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.466s | Acc so far: 99.21%\n",
            "Batch 550/782 | Loss: 0.5370 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.203s | Batch: 0.466s | Acc so far: 99.21%\n",
            "Batch 600/782 | Loss: 0.5007 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.465s | Acc so far: 99.22%\n",
            "Batch 650/782 | Loss: 0.5257 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.205s | Batch: 0.473s | Acc so far: 99.21%\n",
            "Batch 700/782 | Loss: 0.5026 | Data: 0.013s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.469s | Acc so far: 99.21%\n",
            "Batch 750/782 | Loss: 0.5009 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.466s | Acc so far: 99.20%\n",
            "Epoch finished in 374.80s\n",
            "[Eval] Batch 50/157 | Loss: 0.5172 | Acc so far: 98.72%\n",
            "[Eval] Batch 100/157 | Loss: 0.6043 | Acc so far: 98.69%\n",
            "[Eval] Batch 150/157 | Loss: 0.5732 | Acc so far: 98.72%\n",
            "Evaluation finished in 106.64s\n",
            "Epoch 26/30 | Train Loss: 0.5192, Train Acc: 99.19% | Val Loss: 0.5344, Val Acc: 98.75%\n",
            "\n",
            "--- Epoch 27/30 ---\n",
            "Batch 50/782 | Loss: 0.5039 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.464s | Acc so far: 98.75%\n",
            "Batch 100/782 | Loss: 0.5153 | Data: 0.013s | Fwd: 0.017s | Bwd: 0.200s | Batch: 0.466s | Acc so far: 98.95%\n",
            "Batch 150/782 | Loss: 0.5180 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.206s | Batch: 0.471s | Acc so far: 99.03%\n",
            "Batch 200/782 | Loss: 0.5008 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.464s | Acc so far: 99.04%\n",
            "Batch 250/782 | Loss: 0.5010 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.465s | Acc so far: 99.09%\n",
            "Batch 300/782 | Loss: 0.5010 | Data: 0.024s | Fwd: 0.034s | Bwd: 0.182s | Batch: 0.481s | Acc so far: 99.10%\n",
            "Batch 350/782 | Loss: 0.5514 | Data: 0.020s | Fwd: 0.019s | Bwd: 0.197s | Batch: 0.475s | Acc so far: 99.07%\n",
            "Batch 400/782 | Loss: 0.5008 | Data: 0.011s | Fwd: 0.013s | Bwd: 0.207s | Batch: 0.465s | Acc so far: 99.10%\n",
            "Batch 450/782 | Loss: 0.5888 | Data: 0.020s | Fwd: 0.028s | Bwd: 0.188s | Batch: 0.475s | Acc so far: 99.09%\n",
            "Batch 500/782 | Loss: 0.5008 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.467s | Acc so far: 99.12%\n",
            "Batch 550/782 | Loss: 0.5007 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.203s | Batch: 0.468s | Acc so far: 99.11%\n",
            "Batch 600/782 | Loss: 0.5222 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.464s | Acc so far: 99.13%\n",
            "Batch 650/782 | Loss: 0.5006 | Data: 0.012s | Fwd: 0.014s | Bwd: 0.203s | Batch: 0.466s | Acc so far: 99.13%\n",
            "Batch 700/782 | Loss: 0.5008 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.465s | Acc so far: 99.15%\n",
            "Batch 750/782 | Loss: 0.5007 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.463s | Acc so far: 99.16%\n",
            "Epoch finished in 374.59s\n",
            "[Eval] Batch 50/157 | Loss: 0.5109 | Acc so far: 98.69%\n",
            "[Eval] Batch 100/157 | Loss: 0.6084 | Acc so far: 98.70%\n",
            "[Eval] Batch 150/157 | Loss: 0.5598 | Acc so far: 98.72%\n",
            "Evaluation finished in 106.75s\n",
            "Epoch 27/30 | Train Loss: 0.5200, Train Acc: 99.15% | Val Loss: 0.5347, Val Acc: 98.75%\n",
            "\n",
            "--- Epoch 28/30 ---\n",
            "Batch 50/782 | Loss: 0.5596 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.204s | Batch: 0.472s | Acc so far: 99.22%\n",
            "Batch 100/782 | Loss: 0.5032 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.202s | Batch: 0.465s | Acc so far: 99.20%\n",
            "Batch 150/782 | Loss: 0.5008 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.462s | Acc so far: 99.18%\n",
            "Batch 200/782 | Loss: 0.5006 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.204s | Batch: 0.465s | Acc so far: 99.12%\n",
            "Batch 250/782 | Loss: 0.5010 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.463s | Acc so far: 99.12%\n",
            "Batch 300/782 | Loss: 0.5192 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.466s | Acc so far: 99.12%\n",
            "Batch 350/782 | Loss: 0.5013 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.462s | Acc so far: 99.17%\n",
            "Batch 400/782 | Loss: 0.5322 | Data: 0.020s | Fwd: 0.015s | Bwd: 0.197s | Batch: 0.472s | Acc so far: 99.21%\n",
            "Batch 450/782 | Loss: 0.5699 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.461s | Acc so far: 99.21%\n",
            "Batch 500/782 | Loss: 0.5295 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.203s | Batch: 0.471s | Acc so far: 99.23%\n",
            "Batch 550/782 | Loss: 0.5008 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.463s | Acc so far: 99.23%\n",
            "Batch 600/782 | Loss: 0.5150 | Data: 0.022s | Fwd: 0.015s | Bwd: 0.198s | Batch: 0.472s | Acc so far: 99.25%\n",
            "Batch 650/782 | Loss: 0.5209 | Data: 0.025s | Fwd: 0.024s | Bwd: 0.191s | Batch: 0.480s | Acc so far: 99.25%\n",
            "Batch 700/782 | Loss: 0.5014 | Data: 0.030s | Fwd: 0.044s | Bwd: 0.172s | Batch: 0.486s | Acc so far: 99.22%\n",
            "Batch 750/782 | Loss: 0.5330 | Data: 0.011s | Fwd: 0.019s | Bwd: 0.201s | Batch: 0.472s | Acc so far: 99.22%\n",
            "Epoch finished in 374.15s\n",
            "[Eval] Batch 50/157 | Loss: 0.5229 | Acc so far: 98.69%\n",
            "[Eval] Batch 100/157 | Loss: 0.6042 | Acc so far: 98.67%\n",
            "[Eval] Batch 150/157 | Loss: 0.5652 | Acc so far: 98.72%\n",
            "Evaluation finished in 106.34s\n",
            "Epoch 28/30 | Train Loss: 0.5180, Train Acc: 99.23% | Val Loss: 0.5342, Val Acc: 98.75%\n",
            "\n",
            "--- Epoch 29/30 ---\n",
            "Batch 50/782 | Loss: 0.5017 | Data: 0.024s | Fwd: 0.037s | Bwd: 0.180s | Batch: 0.477s | Acc so far: 99.03%\n",
            "Batch 100/782 | Loss: 0.5007 | Data: 0.024s | Fwd: 0.031s | Bwd: 0.184s | Batch: 0.481s | Acc so far: 99.05%\n",
            "Batch 150/782 | Loss: 0.5140 | Data: 0.021s | Fwd: 0.025s | Bwd: 0.191s | Batch: 0.478s | Acc so far: 99.10%\n",
            "Batch 200/782 | Loss: 0.5312 | Data: 0.018s | Fwd: 0.017s | Bwd: 0.200s | Batch: 0.472s | Acc so far: 99.12%\n",
            "Batch 250/782 | Loss: 0.5339 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.462s | Acc so far: 99.12%\n",
            "Batch 300/782 | Loss: 0.5133 | Data: 0.013s | Fwd: 0.016s | Bwd: 0.200s | Batch: 0.470s | Acc so far: 99.14%\n",
            "Batch 350/782 | Loss: 0.5300 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.467s | Acc so far: 99.17%\n",
            "Batch 400/782 | Loss: 0.5009 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.467s | Acc so far: 99.18%\n",
            "Batch 450/782 | Loss: 0.5010 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.199s | Batch: 0.465s | Acc so far: 99.15%\n",
            "Batch 500/782 | Loss: 0.5465 | Data: 0.029s | Fwd: 0.023s | Bwd: 0.190s | Batch: 0.479s | Acc so far: 99.16%\n",
            "Batch 550/782 | Loss: 0.5006 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.201s | Batch: 0.464s | Acc so far: 99.17%\n",
            "Batch 600/782 | Loss: 0.5036 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.202s | Batch: 0.466s | Acc so far: 99.16%\n",
            "Batch 650/782 | Loss: 0.5415 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.199s | Batch: 0.465s | Acc so far: 99.16%\n",
            "Batch 700/782 | Loss: 0.5008 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.464s | Acc so far: 99.19%\n",
            "Batch 750/782 | Loss: 0.5010 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.200s | Batch: 0.462s | Acc so far: 99.20%\n",
            "Epoch finished in 374.68s\n",
            "[Eval] Batch 50/157 | Loss: 0.5251 | Acc so far: 98.72%\n",
            "[Eval] Batch 100/157 | Loss: 0.6033 | Acc so far: 98.67%\n",
            "[Eval] Batch 150/157 | Loss: 0.5689 | Acc so far: 98.72%\n",
            "Evaluation finished in 106.50s\n",
            "Epoch 29/30 | Train Loss: 0.5188, Train Acc: 99.19% | Val Loss: 0.5341, Val Acc: 98.75%\n",
            "\n",
            "--- Epoch 30/30 ---\n",
            "Batch 50/782 | Loss: 0.5563 | Data: 0.014s | Fwd: 0.015s | Bwd: 0.198s | Batch: 0.466s | Acc so far: 99.22%\n",
            "Batch 100/782 | Loss: 0.5329 | Data: 0.024s | Fwd: 0.033s | Bwd: 0.183s | Batch: 0.478s | Acc so far: 99.17%\n",
            "Batch 150/782 | Loss: 0.5365 | Data: 0.012s | Fwd: 0.015s | Bwd: 0.202s | Batch: 0.467s | Acc so far: 99.22%\n",
            "Batch 200/782 | Loss: 0.5006 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.203s | Batch: 0.463s | Acc so far: 99.20%\n",
            "Batch 250/782 | Loss: 0.5135 | Data: 0.012s | Fwd: 0.016s | Bwd: 0.201s | Batch: 0.468s | Acc so far: 99.21%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save best model\n",
        "torch.save(model.state_dict(), \"vit_cifar10.pth\")"
      ],
      "metadata": {
        "id": "54zpKW7hLWoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "                                                                                                                                         # ### Colab Notebook: Vision Transformer (ViT) Fine-Tuning on CIFAR-10\n",
        "# # Goal: Highest accuracy possible with minimal compute (using pretrained ViT)\n",
        "\n",
        "# # --- SETUP ---\n",
        "# !pip install timm torchsummary\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import DataLoader\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "# import timm\n",
        "# from timm.loss import LabelSmoothingCrossEntropy\n",
        "# from timm.utils import ModelEmaV2\n",
        "\n",
        "# # --- CONFIG ---\n",
        "# class CFG:\n",
        "#     model_name = \"vit_base_patch16_224\"  # pretrained ViT\n",
        "#     img_size = 224\n",
        "#     batch_size = 64\n",
        "#     epochs = 30\n",
        "#     lr = 5e-5\n",
        "#     weight_decay = 0.05\n",
        "#     num_classes = 10\n",
        "#     smoothing = 0.1\n",
        "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# cfg = CFG()\n",
        "\n",
        "# # --- DATASET & AUGMENTATIONS ---\n",
        "# train_transform = transforms.Compose([\n",
        "#     transforms.Resize((cfg.img_size, cfg.img_size)),\n",
        "#     transforms.RandomHorizontalFlip(),\n",
        "#     transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "# ])\n",
        "\n",
        "# val_transform = transforms.Compose([\n",
        "#     transforms.Resize((cfg.img_size, cfg.img_size)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "# ])\n",
        "\n",
        "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "# trainloader = DataLoader(trainset, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# valset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=val_transform)\n",
        "# valloader = DataLoader(valset, batch_size=cfg.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# # --- MODEL ---\n",
        "# model = timm.create_model(cfg.model_name, pretrained=True, num_classes=cfg.num_classes)\n",
        "# model.to(cfg.device)\n",
        "\n",
        "# # --- LOSS & OPTIMIZER ---\n",
        "# criterion = LabelSmoothingCrossEntropy(smoothing=cfg.smoothing)\n",
        "# optimizer = optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n",
        "\n",
        "# # EMA for stability\n",
        "# ema = ModelEmaV2(model, decay=0.999)\n",
        "\n",
        "# # --- TRAIN & EVAL ---\n",
        "# def train_one_epoch(model, loader, optimizer, criterion, device, ema):\n",
        "#     model.train()\n",
        "#     total_loss, correct, total = 0, 0, 0\n",
        "#     for images, targets in loader:\n",
        "#         images, targets = images.to(device), targets.to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         with torch.cuda.amp.autocast():\n",
        "#             outputs = model(images)\n",
        "#             loss = criterion(outputs, targets)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         ema.update(model)\n",
        "\n",
        "#         total_loss += loss.item() * images.size(0)\n",
        "#         _, predicted = outputs.max(1)\n",
        "#         total += targets.size(0)\n",
        "#         correct += predicted.eq(targets).sum().item()\n",
        "#     return total_loss/total, 100.*correct/total\n",
        "\n",
        "\n",
        "# def evaluate(model, loader, criterion, device):\n",
        "#     model.eval()\n",
        "#     total_loss, correct, total = 0, 0, 0\n",
        "#     with torch.no_grad():\n",
        "#         for images, targets in loader:\n",
        "#             images, targets = images.to(device), targets.to(device)\n",
        "#             outputs = model(images)\n",
        "#             loss = criterion(outputs, targets)\n",
        "\n",
        "#             total_loss += loss.item() * images.size(0)\n",
        "#             _, predicted = outputs.max(1)\n",
        "#             total += targets.size(0)\n",
        "#             correct += predicted.eq(targets).sum().item()\n",
        "#     return total_loss/total, 100.*correct/total\n",
        "\n",
        "# # --- TRAINING LOOP ---\n",
        "# for epoch in range(cfg.epochs):\n",
        "#     train_loss, train_acc = train_one_epoch(model, trainloader, optimizer, criterion, cfg.device, ema)\n",
        "#     val_loss, val_acc = evaluate(model, valloader, criterion, cfg.device)\n",
        "#     scheduler.step()\n",
        "#     print(f\"Epoch {epoch+1}/{cfg.epochs} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# # Save best model\n",
        "# torch.save(model.state_dict(), \"vit_cifar10.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1oboKpmPiU2r",
        "outputId": "02c495ff-8b03-4c5a-e1d9-b7f76cbc161a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.20)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.12/dist-packages (1.5.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.23.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.35.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.8.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-361119934.py:71: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30 | Train Acc: 93.84% | Val Acc: 96.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-361119934.py:71: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/tmp/ipython-input-361119934.py:71: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/tmp/ipython-input-361119934.py:71: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-361119934.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{cfg.epochs} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-361119934.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, loader, criterion, device)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzapo-rHiJV-"
      },
      "outputs": [],
      "source": []
    }
  ]
}